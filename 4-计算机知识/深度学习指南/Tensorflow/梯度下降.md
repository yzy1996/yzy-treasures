# 梯度下降



使用 `tf.GradientTape`

```python
x = tf.constant(3.0)
with tf.GradientTape() as g:
  g.watch(x)
  y = x * x
dy_dx = g.gradient(y, x) # Will compute to 6.0
```



也可以连续求高阶导

```python
x = tf.constant(3.0)
with tf.GradientTape() as g:
  g.watch(x)
  with tf.GradientTape() as gg:
    gg.watch(x)
    y = x * x
  dy_dx = gg.gradient(y, x)     # Will compute to 6.0
d2y_dx2 = g.gradient(dy_dx, x)  # Will compute to 2.0
```



默认情况下，回调 `GradientTape.gradient() ` 后，`GradientTape ` 的资源会被自动释放，如果想要不被自动释放，使用参数 `persistent=True` 。但最后也需要手动释放掉

```python
x = tf.constant(3.0)
with tf.GradientTape(persistent=True) as g:
  g.watch(x)
  y = x * x
  z = y * y
dz_dx = g.gradient(z, x)  # 108.0 (4*x^3 at x = 3)
dy_dx = g.gradient(y, x)  # 6.0
del g  # Drop the reference to the tape
```



你可能想问都需要 `watch` 变量吗？如果是默认 `trainable=True` 的 Trainable variables (例如 `tf.Variable`) 就不需要额外的 `watch` 