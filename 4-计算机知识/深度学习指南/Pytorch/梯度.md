# 梯度

> 看知乎

PyTorch提供两种实现方法：`backward` and  `autograd.grad`



但不管是哪一种，首先我们都需要给想要求梯度的变量加上 `require`

`Tensor`变量有一个属性 `.requires_grad`，所以要想保留梯度需要额外的指示





### `backward`





### zero_grad()

通常会有 `model.zero_grad()` 和 `optimizer.zero_grad()` 两种

这两种都是为了把模型中参数的梯度设为0

当 `optimizer = optim.Optimizer(net.parameters()) `时，两者是等效的



### detach()

```python
x = torch.randn(2, 2)
x.requires_grad = True

lin0 = nn.Linear(2, 2)
lin1 = nn.Linear(2, 2)
lin2 = nn.Linear(2, 2)
lin3 = nn.Linear(2, 2)
x1 = lin0(x)
x2 = lin1(x1)
x2 = x2.detach() # 此处设置了detach，之前的所有梯度流都不会回传了
x3 = lin2(x2)
x4 = lin3(x3)
x4.sum().backward()
print(lin0.weight.grad)
print(lin1.weight.grad)
print(lin2.weight.grad)
print(lin3.weight.grad)
```



事实上，我们可以通过设置张量的requires_grad属性来设置某个张量是否计算梯度，而这个不会影响梯度回传，只会影响当前的张量。修改上面的代码，我们有：



```python
x = torch.randn(2, 2)
x.requires_grad = True

lin0 = nn.Linear(2, 2)
lin1 = nn.Linear(2, 2)
lin2 = nn.Linear(2, 2)
lin3 = nn.Linear(2, 2)
x1 = lin0(x)
x2 = lin1(x1)
for p in lin2.parameters():
    p.requires_grad = False
x3 = lin2(x2)
x4 = lin3(x3)
x4.sum().backward()
print(lin0.weight.grad)
print(lin1.weight.grad)
print(lin2.weight.grad)
print(lin3.weight.grad)
```





---

在pytorch的计算图里只有两种元素：数据（tensor）和 运算（operation）

数据可分为：









非叶子节点的tensor，如果想要获取他们的梯度，需要使用 `retain_grad()`





loss.backward() will compute the gradient

optimizer.step() will apply the gradient and update the tensor.

```python
loss1.backward(retain_graph=True)
loss2.backward(retain_graph=False)
optimizer.step()
optimizer2.step()
```





modified by an inplace operation



使用 **anomaly_detection**

```python
import torch


with torch.autograd.set_detect_anomaly(True):
    a = torch.rand(1, requires_grad=True)
    c = torch.rand(1, requires_grad=True)

    b = a ** 2 * c ** 2
    b += 1
    b *= c + a

    d = b.exp_()
    d *= 5

    b.backward()
    
sys:1: RuntimeWarning: Traceback of forward call that caused the error:
  File "tst.py", line 13, in <module>
    d = b.exp_()

Traceback (most recent call last):
  File "tst.py", line 16, in <module>
    b.backward()
  File "/Users/fmassa/anaconda3/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/Users/fmassa/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py", line 93, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation
```











```python
loss.backward() 会计算当前参数 p0 经过前向过程 a0 后的各 tensor 的梯度
optimizer.step() 会更新参数 p0 -> p1

如果再次调用 loss.backward()
但loss 是在前p0基础上计算的，此刻又会要求在p1基础上计算，就产生了问题

you could either recalculate the forward pass to create the forward activations using the already updated parameters or update the parameters after all gradients were computed.
```



关于pytorch中使用detach并不能阻止参数更新这档子事儿

https://zhuanlan.zhihu.com/p/344916574

zero_grad将计算得的梯度变为全零张量，而不是变为之前的None。只要将梯度再次转化为None，那么就会避免历史信息和weight_decay对参数的更新了。pytorch已经考虑到了这一点，调用zero_grad时可以指定参数：set_to_none，将其置为True即可。









在pytorch中停止梯度流的若干办法，避免不必要模块的参数更新

https://cloud.tencent.com/developer/article/1807439

https://www.cxybb.com/article/qq_36429555/118547133

1、停止计算某个模块的梯度，在优化过程中这个模块还是会被考虑更新，然而因为梯度已经被截断了，因此不能被更新。

设置tensor.detach()： 完全截断之前的梯度流
设置参数的requires_grad属性：单纯不计算当前设置参数的梯度，不影响梯度流
torch.no_grad()：效果类似于设置参数的requires_grad属性
2、在优化器中设置不更新某个模块的参数，这个模块的参数在优化过程中就不会得到更新，然而这个模块的梯度在反向传播时仍然可能被计算。

