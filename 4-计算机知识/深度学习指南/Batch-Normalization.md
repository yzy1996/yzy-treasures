对 DNN 中每一个 Activation，在它们前面放置一个 BN Layer（Batch Normalization Layer）。相当于以前的将 ![[公式]](https://www.zhihu.com/equation?tex=Wu%2Bb) 输入 Activation Function，现在将BN( ![[公式]](https://www.zhihu.com/equation?tex=Wu%2Bb) )输入 Activation Function



