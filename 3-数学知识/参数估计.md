#! https://zhuanlan.zhihu.com/p/592423240
# 参数估计

先给出以下定义：

先验分布：$P(\theta)$ 

似然函数：$P(D | \theta)$ 

后验分布：$P(\theta | D)$

贝叶斯公式：$P(\theta | D)=\frac{P(D | \theta) P(\theta)}{P(D)}$  其中 $P(D)=\sum_{i=1}^{n} P(x_i|\theta)P(\theta)$ 

这样我们很容易得到他们四者的关系：后验分布 = 似然函数 × 先验分布 / P(D)



接着再用一个例子来帮助理解

>路人甲在一个不透明的袋里放了若干个黑色和白色的球，他感觉是白色球更多；路人乙想知道袋中球的情况，就从袋中有放回式取球，一共取了10次，有7次是白球，3次是黑球。

通过这个例子，白球或黑球的比例就是 $\theta$ （$0 \leq \theta \leq 1$） ，路人甲的感觉就是先验分布 $P(\theta)$ ，路人乙观测到的就是 $D=\{x_1, x_2... x_{10}\}$ （其中$x_i$ = 1(白色) or 0(黑色)），路人乙通过观测想知道比例就是似然 $P(D | \theta)$ ，路人乙在知道路人甲的感觉后想知道比例就是后验分布 $P(\theta | D)$ 



当知道观测值 $x$ 时，估计参数 $\theta$ 就是<u>参数估计</u>；当知道参数 $\theta$ 时，预测观测值 $x$ 就是<u>预测</u>；**参数估计的目的时为了对新的观测值进行预测**。



有了初步的理解认识后，我们开始介绍正题

## 1、最大似然估计（MLE）

Maximum Likelihood Estimation

最大似然估计的思想是：认为参数是一个固定的值，找到使得发生观测数据情况概率最大的参数，即让似然函数最大的参数。

似然函数表示如下：
$$
L\left(\theta | x_{1}, x_{2}, \cdots, x_{n}\right)=f\left(x_{1}, x_{2}, \cdots, x_{n} | \theta\right)=\prod_{i=1}^{n} f\left(x_{i} | \theta\right)
$$
因为连乘不好求解，通常会进行一个对数变换，转换为累加：
$$
l(\theta) = \log L\left(\theta | x_{1}, x_{2}, \cdots, x_{n}\right)=\sum_{i=1}^{n} log[f\left(x_{i} | \theta\right)]
$$


### 推导

#### 伯努利分布（Bernoulli）

每一个样本的概率可以表示为：$P(x|\theta)=\theta^{x}(1-\theta)^{1-x}$ ，$x=0 \ \text{or} \ 1$  ，$\theta$ 是成功的概率

我们现在有n个样本，$D = \{ x_1, x_2,...,x_n \}$ ，

写出对数最大似然函数表达式：
$$
\begin{align}
l(\theta)&=\sum_{i=1}^{n}\log{P(x_i|\theta)} \\
  &= \sum_{i=1}^{n}\log{\theta^{x_i}(1-\theta)^{1-x_i}} \\
  &= \sum_{i=1}^{n}x_i \log{\theta}+(1-x_i)\log(1-\theta) \\
  &= (\sum_{i=1}^{n}x_i)\log{\theta}+(\sum_{i=1}^{n}(1-x_i))\log{(1-\theta)} \\
  &= m\log{\theta} + (n-m)\log(1-\theta)
\end{align}
$$
对 $l(\theta)$ 进行求导，解得当 $\theta = \frac{1}{n}\sum_{i=1}^{n}x_i$ ，似然函数 $l(\theta)$ 取最大值

#### 正态分布（Gaussian）

每一个样本的概率可以表示为： $P(x|\theta )=\frac{1}{\sqrt{2 \pi} \sigma}\exp(-\frac{(x-\theta)^{2}}{2 \sigma^{2}})$  

所以对数最大似然函数表达式：
$$
\begin{align}
l(\theta)&=\sum_{i=1}^{n}\log{P(x_i|\theta)} \\
  &= \sum_{i=1}^{n} [-\frac{1}{2}log{2\pi}-log{\sigma}-\frac{(x_i-\theta)^2}{2\sigma^2}] \\
  &= -\frac{n}{2}log{2\pi}-nlog{\sigma}-\frac{\sum_{i=1}^{n}(x_i-\theta)^2}{2\sigma^2}
\end{align}
$$
对 $l(\theta)$ 进行求导，解得当 $\theta = \frac{1}{n}\sum_{i=1}^{n}x_i$ ，似然函数 $l(\theta)$ 取最大值

### 例子

> 问：我们还是用开头的例子，路人甲在一个不透明的袋里放了若干个黑色和白色的球；路人乙想知道袋中球的情况，就从袋中有放回式取球，一共取了10次，有7次是白球，3次是黑球。问白球的比例是多少？

> 答：在这个例子中，每一次实验都是服从伯努利分布，我们设白球的概率为 $\theta$ ，所以每一次实验可以表示为： $P(x_i|\theta)=\theta^{x_i}(1-\theta)^{1-x_i}$  。样本为 $D = \{ x_1, x_2,...,x_{10} \}$ 
>
> 写出似然函数：$P(D|\theta) = \prod_{i=1}^{10} P(x_i|\theta)={\theta}^{7}(1-\theta)^{3}$
>
> 求出上面似然函数最大时的 $\theta$ 值（先取对数），得到 $\theta = 0.7$



### 计算步骤

1. 选择参数模型（伯努利、高斯...），设参数为 $\theta$
2. 获得样本数据  $D = \{ x_1, x_2, x_3,...,x_{n} \}$ 
3. 通过最大似然公式求解



## 2、最大后验估计(MAP)

Maximum A Posteriori Estimation

在最大似然估计的例子中，如果样本数量不够多，其实存在着很大的问题

MLE简单又客观，但是过分的客观有时会导致过拟合(Over fitting)。在样本点很少的情况下，MLE的效果并不好

> 一个最简单的例子就是，一个伯努利模型，我们知道通过最大似然估计得到的先验值为 $\frac{1}{N}\sum_{i=1}^{N}{x_i}$ ，那如果实验过程中，全部出现1或者0，那么估计出的参数显然是不对的

最大似然估计认为使似然函数 $P(D|θ)$ 最大的参数 $θ$ 即为最好的 $θ$ ；最大后验估计认为使 $P(X|θ)P(θ)$ 最大的参数 $θ$ 即为最好的 $θ$ ；最大似然估计可以看作是一种特殊的最大后验估计，将 $\theta$ 看作是固定的， $P(\theta)=1$ 。

最大后验概率估计的公式表示：（P(D)是一个常数，与 $\theta$ 无关）
$$
\underset{\theta}{\operatorname{argmax}} P(\theta|D)=
\underset{\theta}{\operatorname{argmax}} \frac{P(D|\theta) P(\theta)}{P(D)} 
\propto 
\underset{\theta}{\operatorname{argmax}} P(D|\theta) P(\theta)
$$

### 推导

要求解MAP，还需要知道参数的先验分布，

#### 正态分布

我们假设高斯分布方差已知，现在要估计均值，将均值记为 $\theta$ 

每一个样本的概率可以表示为 $P(x|\theta )=\frac{1}{\sqrt{2 \pi} \sigma}\exp(-\frac{(x-\theta)^{2}}{2 \sigma^{2}})$  

我们假设 $\theta$ 服从高斯分布，$\theta \sim N(\mu_0,\sigma^2_0)$  则：$P(\theta) = \frac{1}{\sqrt{2 \pi} \sigma_0} exp({-\frac{(\theta-\mu_0)^{2}}{2 \sigma^{2}_0}})$
$$
\begin{align}
\underset{\theta}{\operatorname{argmax}} P(D|\theta) P(\theta) 
&=\underset{\theta}{\operatorname{argmax}} log{P(D|\theta)}+logP(\theta) \\
&=\underset{\theta}{\operatorname{argmax}} [-\frac{n}{2}log{2\pi}-nlog{\sigma}-\frac{\sum_{i=1}^{n}(x_i-\theta)^2}{2\sigma^2}]
-\frac{1}{2}log{2\pi}-log{\sigma_0}-\frac{(\theta-\mu_0)^2}{2\sigma_0^2} 
\end{align}
$$

求解上式得到 $\theta = \frac{\sigma_0^2 \sum_{i=1}^{n} x_{i} - \sigma^2\mu_0}{\sigma_0^2 n -\sigma^2}$ 

### 例子

> 问：我们还是用开头的例子，路人甲在一个不透明的袋里放了若干个黑色和白色的球，他感觉白色球更多；路人乙想知道袋中球的情况，就从袋中有放回式取球，一共取了10次，有7次是白球，3次是黑球。问白球的比例是多少？

> 答：在这个例子中，每一次实验都是服从伯努利分布，我们设白球的概率为 $\theta$ ，所以每一次实验可以表示为： $P(x_i|\theta)=\theta^{x_i}(1-\theta)^{1-x_i}$  。样本为 $D = \{ x_1, x_2,...,x_{10} \}$ 。我们还知道路人甲的感觉是白色球更多，我们需要给出一个 $\theta$ 的分布，假设 $P(\theta) = 2\theta$  
>
> 写出后验概率函数：$P(D|\theta)P(\theta) = \prod_{i=1}^{10} p(x_i|\theta) × 2\theta=2{\theta}^{8}(1-\theta)^{3}$ 
>
> 求出上面后验概率最大时的 $\theta$ 值（先取对数），得到 $\theta = 0.73$



**当样本个数无穷多的时候，MAP上会逼近MLE，因为样本足够多了，就不需要先验了，或者比起先验更相信样本**。



## 3、贝叶斯估计

最大似然估计和最大后验估计都是估计了参数的具体值，但更令人信服的其实是参数的分布，知道参数在取每个值时的概率。

与最大后验估计一样，需要用到贝叶斯定理：
$$
P(\theta | D)=\frac{P(D | \theta) P(\theta)}{\int_{\Theta} P(D | \theta) P(\theta) d \theta}
$$
同样的我们需要知道先验分布 $P(\theta)$ ，但此时不再求 $\underset{\theta}{\operatorname{argmax}}P(\theta|D)$ ，而要求出 $P(\theta|D)$ 。这里如果先验分布十分复杂，上式会很难求解（因为要分母积分），所以一般会选择共轭先验。

最大后验估计和贝叶斯估计也存在一个问题，实际应用场景中的先验概率不是那么好求，很多都是拍脑袋决定的。一旦是拍脑袋决定的，自然也就不准了，先验概率不准，那么计算出的后验概率也就相应的不准了。

贝叶斯估计用来预测新测量数据的概率，对于新出现的数据 $x$ :
$$
P({x} | D)=\int_{\Theta} P({x} | \theta) P(\theta | D) d \theta=\int_{\Theta} P({x} | \theta) \frac{P(D | \theta) P(\theta)}{P(D)} d \theta
$$

#### 例子

> 问：我们还是用开头的例子，路人甲在一个不透明的袋里放了若干个黑色和白色的球，他感觉白色球更多；路人乙想知道袋中球的情况，就从袋中有放回式取球，一共取了10次，有7次是白球，3次是黑球。问白球的比例是多少？
>
> 答：在这个例子中，每一次实验都是服从伯努利分布，我们设白球的概率为 $\theta$ ，所以每一次实验可以表示为： $P(x_i|\theta)=\theta^{x_i}(1-\theta)^{1-x_i}$  。样本为 $D = \{ x_1, x_2,...,x_{10} \}$ 。我们还知道路人甲的感觉是白色球更多，我们需要给出一个 $\theta$ 的分布，假设 $\theta \sim \operatorname{Beta}(\alpha, \beta)$ 
> $$
> f(\theta ; \alpha, \beta)= \frac{\theta^{\alpha-1}(1-\theta)^{\beta-1}}{\int_{0}^{1} u^{\alpha-1}(1-u)^{\beta-1} d u} = \frac{1}{B(\alpha, \beta)} \theta^{\alpha-1}(1-\theta)^{\beta-1}
> $$
> 
>
> 写出后验概率函数：
> $$
> \begin{aligned} 
> P(\theta | D) 
> &=\frac{P(D | \theta) P(\theta)}{\int_{\Theta} P(D | \theta) P(\theta) d \theta} \\ 
> &=\frac{\theta^{7}(1-\theta)^{3} \frac{\theta^{\alpha-1}(1-\theta)^{\beta-1}}{B(\alpha, \beta)}}{\int_{\Theta} \theta^{7}(1-\theta)^{3} \frac{\theta^{\alpha-1}(1-\theta)^{\beta-1}}{B(\alpha, \beta)}d \theta} \\ 
> &=\frac{\theta^{\alpha+7-1}(1-\theta)^{\beta+3-1}}{\int_{\Theta} \theta^{\alpha+7-1}(1-\theta)^{\beta+3-1} d \theta} \\ 
> &=\frac{\theta^{\alpha+6}(1-\theta)^{\beta+2}}{B(\alpha+6, \beta+2)} \\ &=\operatorname{Beta}(\theta | \alpha+6, \beta+2)
> \end{aligned}
> $$
> 所以我们得到了参数的后验分布情况，得到 $\theta \sim \operatorname{Beta}(\alpha+6, \beta+2)$ 

### 计算步骤

1、计算后验概率 $P(\theta|D)$

根据贝叶斯定理：
$$
P(\theta | D)=\frac{P(D | \theta) P(\theta)}{\int_{\Theta} P(D | \theta) P(\theta) d \theta}
$$
2、计算新样本的估计
$$
P(x|D) = \int_{\Theta} P(x | \theta) P(\theta | D) d \theta
$$



## 总结

MLE、MAP是选择相对最好的一个模型， 贝叶斯方法则是通过观测数据来估计后验分布，并通过后验分布做群体决策，所以后者的目标并不是在去选择某一个最好的模型，而是去评估每一个模型的好坏。



## 知识点

* 概率是已知参数，对结果可能性的预测。似然是已知结果，对参数是某个值的可能性预测

* 二项分布参数的共轭先验是Beta分布，多项式分布参数的共轭先验是Dirichlet分布，指数分布参数的共轭先验是Gamma分布，⾼斯分布均值的共轭先验是另⼀个⾼斯分布，泊松分布的共轭先验是Gamma分布。

  

## 参考

[叶斯估计、最大似然估计、最大后验估计三者的区别](https://yuanxiaosc.github.io/2018/06/20/贝叶斯估计、最大似然估计、最大后验估计三者的区别/)

[贝叶斯估计、最大似然估计、最大后验概率估计](http://noahsnail.com/2018/05/17/2018-05-17-%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1%E3%80%81%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E3%80%81%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%E4%BC%B0%E8%AE%A1/)