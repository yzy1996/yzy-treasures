# åˆ©æ™®å¸ŒèŒ¨è¿ç»­æ€§ (Lipschitz Continuity)



ä¸»è¦å…¬å¼

å­˜åœ¨å¸¸æ•°Kï¼Œä½¿å¾—
$$
\|D(x)-D(y)\| \leq K\|x-y\|
$$
å¯¹äºå®æ•°
$$
\frac{f(x)-f(y)}{x-y} \leq K
$$
æœ€å°çš„Kè¢«ç§°ä¸ºåˆ©æ™®å¸ŒèŒ¨å¸¸æ•°

> æ³¨æ„ä¸éœ€è¦å¯å¯¼



## ä¾‹å­

- $f(x) = x^2$ åœ¨å®šä¹‰åŸŸ $R$ ä¸Šä¸æ»¡è¶³åˆ©æ™®å¸ŒèŒ¨æ¡ä»¶ï¼Œåœ¨æœ‰é™å®šä¹‰åŸŸä¸Šæ»¡è¶³
- $f(x) = \sqrt{x^2+5}$ æ»¡è¶³åˆ©æ™®å¸ŒèŒ¨æ¡ä»¶ï¼Œ $K=1$
- $f(x) = |x|$ æ»¡è¶³åˆ©æ™®å¸ŒèŒ¨æ¡ä»¶ï¼Œ $K=1$







1-Lipschitz activation functions (e.g., ReLU) 





A dominant strategy to achieve Lipschitz regularization is to perform weight normalization.



or instance, if one wants to enforce the network to be 1-Lipschitz ğ‘ = 1, then one can achieve this by normalizing the weight such that âˆ¥Wğ‘– âˆ¥ğ‘ = 1 after each gradient step during training.


$$
\begin{gathered}
\|M\|_2=\sigma_{\max }(\mathrm{M}), \\
\|\mathrm{M}\|_1=\max _j \sum_i\left|m_{i j}\right|, \\
\quad\|M\|_{\infty}=\max _i \sum_j\left|m_{i j}\right|,
\end{gathered}
$$
where ğœ_max (M) denotes the maximum eigenvalue of M. Thus, when ğ‘ = 2, weight normalization consists of rescaling the weight matrix based on its maximum eigenvalue. Other popular techniques include spectral normalization based on the power iteration and the BjÃ¶rck Orthonormalization. When ğ‘ = âˆ (ğ‘ = 1), weights normalization is simply scaling individual rows (columns) to have a maximum absolute row (column) sum smaller than a prescribed bound.
$$
\begin{aligned}
& \frac{1}{\sqrt{n}}\|M\|_{\infty} \leq\|M\|_2 \leq \sqrt{m}\|M\|_{\infty} \\
& \frac{1}{\sqrt{m}}\|M\|_1 \leq\|M\|_2 \leq \sqrt{n}\|M\|_1
\end{aligned}
$$



$$
\mathrm{y}=\sigma\left(\widehat{\mathrm{W}}_i \mathrm{x}+\mathrm{b}_i\right), \quad \widehat{\mathrm{W}}_i=\text { normalization }\left(\mathrm{W}_i \text {, softplus }\left(c_i\right)\right)
$$
here the softplus (ğ‘_ğ‘–) = ln(1+ğ‘’^(c_i) ) is a reparameterization designed to avoid infeasible negative Lipschitz bounds.





```python
import jax.numpy as jnp

def normalization(Wi, softplus_ci): # L-inf norm
  absrowsum = jnp.sum(jnp.abs(Wi), axis=1)
  scale = jnp.minimum(1.0, softplus_ci/absrowsum)
  return Wi * scale[:,None]

## and each layer of the Lipscthiz MLP is simply
y = sigma(normalization(Wi, softplus(ci))*x + bi)

## where sigma denotes the activation function and softplus is the
## built-in softplus function in JAX.
```



- [Adversarial Lipschitz Regularization](https://arxiv.org/abs/1907.05681)  
  **[`ICLR 2020`]** *DÃ¡vid TerjÃ©k*
- 





ä¸€å¼€å§‹ï¼Œä½¿ç”¨explicit Lipschitz penaltyæ˜¯ä¸åˆ‡å®é™…çš„ï¼Ÿ

åœ¨GANé‡Œé¢Wasserstein distance estimation requires the function space of the critic to only consist of1-Lipschitz functions.

å…¶å®æ˜¯gradient norm penalizationï¼Œä¹Ÿæœ‰å¾ˆå¤šåç»­å·¥ä½œ

æœ¬æ–‡æå‡º **enablingthe training of neural networks with regularization terms penalizing the violation of theLipschitz constraint explicitly, instead of through the norm of the gradient.**





Learning Smooth Neural Functions via Lipschitz Regularization





è¿™é‡Œæˆ‘ä»¬æ¥çœ‹å®ƒæ˜¯å¦‚ä½•è®©Neural Fieldså˜å¾—å¹³æ»‘çš„





æœ‰entropic regularization çš„dualé—®é¢˜æ˜¯smoothçš„ã€‚