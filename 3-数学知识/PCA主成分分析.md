# 主成分分析

> Principal Component Analysis (PCA)
>
> 降维算法之一



## 概述

> 这一部分解释**什么是主成分**，**为什么要找主成分**以及**如何找主成分**



（[维基百科](https://zh.wikipedia.org/wiki/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90)）为了保留数据中对<u>方差</u>贡献最大的特征。它利用正交变换来对一系列可能相关的变量的观测值进行线性变换，从而投影为一系列线性不相关变量的值，这些不相关变量称为主成分。



补充介绍一下什么是信噪比

> 在信号处理中存在**原始信号**和**噪声**两种
>
> 1. 噪声产生：处理过程中设备自行产生的，与原始信号无关，没原始信号时，就存在这样的噪声
>
> 2. 噪声特性：无规律，不随原始信号的变化而变化
>
> 3. 我们的目的：希望尽可能的只有原始信号，而没有噪声
> 4. 引入信噪比：=原始信号/噪声，音频信号一般用功率，其他信号一般用方差
>
> 5. 结论：信噪比越大，说明混在原始信号里的**噪声影响越小**，相比较而言**原始信号**具有**较大**的方差，**噪声**具有**较小**的方差



**理解为什么，可以用几个角度**

- 数据的主成分可以被看成原始信号，非主成分被看成噪声，因为他们是混杂在一起的，所以要想办法把原始信号（主成分）找出来。

- 在做数据分类或回归时，需要提取特征，特征有很多维，但不是每一维都有用，目的要找哪几维（主成分）是最重要的，是最明显的。

> 最有用的数据往往是差异性最大的，想象一下如果所有数据都聚集在一起，还能找到数据的特征规律吗？而这个差异性就用方差来衡量。



**如何找呢**

绕着数据中心旋转原坐标轴，每一个角度上都可以计算方差，找到方差最大的方向作为新的坐标轴

如果是多维的坐标轴，同时还希望不同维度能表示不重复的信息，即希望各维度线性无关

这样一个接一个找，直到找到前 $K$ 个主成分





## 图解例子







## 理论推导

> 过程中统一省略了零均值化，和求和平均

现在有 $N$ 个原始数据 $\{x^{(1)}, x^{(2)}, \dots, x^{(N)}\}$，其中每一个都是 $M$ 维的向量，记作 $X$：
$$
X=\left(\begin{array}{llll}
x^{(1)}_1 & x^{(2)}_1 & \cdots & x^{(N)}_1 \\
x^{(1)}_2 & x^{(2)}_2 & \cdots & x^{(N)}_2 \\
& & \dots \\
x^{(1)}_M & x^{(2)}_M & \cdots & x^{(N)}_M
\end{array}\right)
$$

希望将它降成 $K$ 维新数据，记作 $Y$：
$$
Y=\left(\begin{array}{llll}
x^{\prime(1)}_1 & x^{\prime(2)}_1 & \cdots & x^{\prime(N)}_1 \\
x^{\prime(1)}_2 & x^{\prime(2)}_2 & \cdots & x^{\prime(N)}_2 \\
& & \dots \\
x^{\prime(1)}_K & x^{\prime(2)}_K & \cdots & x^{\prime(N)}_K
\end{array}\right)
$$
这个过程其实是一个基变换，记作 $A$：
$$
Y = A X
$$
根据前面的概述，$Y$ 需要满足两个条件：

- 各个维度的方差最大
- 不同维度之间协方差为0



如何能表示方差和协方差呢，**协方差矩阵**！对角线元素为各个维度的方差，其余为不同维度之间的协方差。



原始数据 $X$ 的协方差矩阵为：
$$
XX^{\mathsf{T}} = \left(\begin{array}{llll}
\sum_{i=1}^N {x^{(i)}_1}^2 & \sum_{i=1}^N {x^{(i)}_1}{x^{(i)}_2} & \cdots & \sum_{i=1}^N {x^{(i)}_1}{x^{(i)}_M} \\
\sum_{i=1}^N {x^{(i)}_2}{x^{(i)}_1} & \sum_{i=1}^N {x^{(i)}_2}^2 & \cdots & \sum_{i=1}^N {x^{(i)}_2}{x^{(i)}_M} \\
& & \dots \\
\sum_{i=1}^N {x^{(i)}_M}{x^{(i)}_1} & \sum_{i=1}^N {x^{(i)}_M}{x^{(i)}_2} & \cdots & \sum_{i=1}^N {x^{(i)}_M}^2
\end{array}\right)
$$
新数据 $Y$ 的协方差矩阵为：
$$
Y Y^{\mathsf{T}} = AX (AX)^{\mathsf{T}} = A (X X^{\mathsf{T}}) A^{\mathsf{T}}
$$
前面提到的两个条件现在需要由 $Y Y^{\mathsf{T}}$ 来满足，即希望得到形如：
$$
Y Y^{\mathsf{T}}=\left(\begin{array}{llll}
\lambda_1 & 0 & \cdots & 0 \\
0 & \lambda_2 & \cdots & 0 \\
& & \dots \\
0 & 0 & \cdots & \lambda_K
\end{array}\right)
$$
看到这不禁想到了**矩阵的特征分解**，可以参考[链接](https://crazyang.blog.csdn.net/article/details/100540556)，对 $X X^{\mathsf{T}}$ 进行分解，$Y Y^{\mathsf{T}}$ 是特征值构成的矩阵，$A$ 是特征向量构成的矩阵。

> 为了方便理解，做一步变换，这里需要用到---因为 $A$ 是正交矩阵，所以 $A^{-1} = A^{\mathsf{T}}$

$$
X X^{\mathsf{T}} = A^{-1} (Y Y^{\mathsf{T}}) (A^{\mathsf{T}})^{-1} = A^{\mathsf{T}} (Y Y^{\mathsf{T}}) (A^{\mathsf{T}})^{-1}
$$

这就和标准的特征分解形式一致了，只是需要分解特征向量矩阵转置一下。



**反思：这个过程中我们好像没有去强调方差最大这个问题呀，这里面欠缺的逻辑其实是矩阵的特征向量和特征值是这个矩阵本身的固有性质，分解出来的特征值就是最大的，特征向量就是正交的。**



目前有两种主流的解释：

- 最大方差理论
- 最小化降维造成的损失



## 算法实现

1. 每一维数据进行零均值化
2. 写出原数据的协方差矩阵
3. 求出协方差矩阵的特征值及对应的特征向量
4. 将特征向量按对应特征值大小从上到下按行排列成矩阵，取前 $K$ 行组成新基矩阵 $A$
5. $Y = A X$ 即为新数据



代码在matlab以及python中有现存包，使用方法如下：

```pythopn

```



## 如何确定主成分个数$K$

> 补充一个点

数据整体方差：$\frac{1}{N} \sum_{i=1}^N \|x^{i}\|^2$

数据映射方差：$\frac{1}{N} \sum_{i=1}^N \|x^{i} - x^{i}_{projection}\|^2$


$$
\frac{\frac{1}{N} \sum_{i=1}^N \|x^{i} - x^{i}_{projection}\|^2}{\frac{1}{N} \sum_{i=1}^N \|x^{i}\|^2} \le t
$$

选取能够满足该式的最小 $K$ 值，$t$ 是阈值范围，$t=0.01$ 表示保留了99%的主要信息





> **最后一句总结：主成分是数据本身的一个固有性质，不是为了找而找，而是它就在那里。**





背后的原理是需要数据满足

对分析起重要作用的成分方差大且数目不是很多，同时噪声的方差较小





最近读到ICLR2021上一篇新的论文-[EigenGame: PCA as a Nash Equilibrium](https://arxiv.org/pdf/2010.00554.pdf) (来自Deepmind)



The principal components (PCs) of data are the vectors that align with the directions of maximum variance. These have two main purposes: 

- as interpretable features 
- for data compression



主要亮点在于从一个新颖的角度看待了$k$-PCA，传统方法都是从优化的角度找出这k个主向量，而这篇文章将他们看成玩家，最终要达到一个纳什平衡

