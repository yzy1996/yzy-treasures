# 梯度消失和爆炸

gradient vanishing and gradient exploding

![NN](C:\Users\zhiyuyang4\Desktop\NN.png)

神经网络扮演了个什么样的角色？

> 就是个非线性函数，一个映射关系，深度神经网络就是由多个这样的非线性层堆叠
> $$
> f(x) = f_n(\dots f_3(f_2(f_1(x)*\theta_1+b_1)*\theta_2+b_2)\dots)
> $$

最终目的是希望这个函数的映射足够正确，假设输出的真值是 $y$，神经网络找到最合适的权值，满足损失最小，例如
$$
Loss = \|y-f(x)\|_{2}^{2}
$$
寻找最小值的方法就是采用梯度下降的方法



反向传播的思想

> 



什么是梯度消失和爆炸？

> 

不同的层学习的速度差异很大，靠近输出的层学习更快，靠近输入的层学习的很慢